
<!DOCTYPE html>
<html>
<head>
<title>MLflow Experiment Report</title>
<style>
table {
  border-collapse: collapse;
  width: 100%;
}
th, td {
  border: 1px solid black;
  padding: 8px;
  text-align: left;
}
th {
  background-color: #f2f2f2;
}
pre {
  white-space: pre-wrap;
  word-wrap: break-word;
}
.indent {
  margin-left: 20px;
}
</style>
</head>
<body>
<h2>MLflow Experiment Report</h2>
<h3>Problem Statement</h3>
<pre class="indent">
    Train a Xgboost model with hyperparameter tuning 
    </pre>
<h3>Data Snippet</h3>
<pre class="indent">
   0	1	0	3	Braund, Mr. Owen Harris	male	22	1	0	A/5 21171	7.25		S
1	2	1	1	Cumings, Mrs. John Bradley (Florence Briggs Thayer)	female	38	1	0	PC 17599	71.2833	C85	C
2	3	1	3	Heikkinen, Miss. Laina	female	26	0	0	STON/O2. 3101282	7.925		S
3	4	1	1	Futrelle, Mrs. Jacques Heath (Lily May Peel)	female	35	1	0	113803	53.1	C123	S
4	5	0	3	Allen, Mr. William Henry	male	35	0	0	373450	8.05		S
    </pre>
<h3>Approach</h3>
<pre class="indent">
   The data analyst's approach to training an Xgboost model with hyperparameter tuning is quite comprehensive and covers most of the necessary steps. However, there are a few areas where the approach could be refined. Here are my suggestions:

1. Data Preprocessing

The data preprocessing step looks good. However, I would add one more step - data scaling. Xgboost is not very sensitive to feature scaling, but it can still improve the model's performance. Therefore, I would suggest scaling the numerical features before training the Xgboost model.

2. Feature Selection

The feature selection step also looks good. However, instead of removing features that are highly correlated with each other, I would suggest using feature selection techniques like Recursive Feature Elimination (RFE) or LASSO regression. These techniques not only help in removing correlated features but also help in identifying the most relevant features for the model.

3. Data Splitting

The data splitting step looks good. However, instead of using a fixed ratio for splitting the dataset, I would suggest using cross-validation techniques like k-fold cross-validation. This technique not only helps in evaluating the model's performance but also helps in hyperparameter tuning.

4. Hyperparameter Tuning

The hyperparameter tuning step looks good. However, instead of using Grid Search or Random Search, I would suggest using Bayesian Optimization techniques like Hyperopt or Optuna. These techniques not only help in finding the best set of hyperparameters but also help in reducing the number of hyperparameter combinations.

5. Model Training

The model training step looks good. However, I would suggest using early stopping techniques like Cross-Validation with Early Stopping to prevent overfitting.

Alternative Approach:

An alternative approach to training an Xgboost model could be using a Neural Network (NN) model. NN models are powerful machine learning algorithms that can learn complex patterns in the data. Here are the steps for training a NN model:

1. Data Preprocessing: Preprocess the data by checking for missing values, converting categorical variables into numerical variables, and scaling the numerical features.
2. Feature Selection: Analyze the correlation between each feature and the target variable. Use feature selection techniques like RFE or LASSO regression to identify the most relevant features for the model.
3. Data Splitting: Split the dataset into training and testing sets using cross-validation techniques like k-fold cross-validation.
4. Model Training: Train the NN model on the training set using techniques like early stopping and regularization.
5. Model Evaluation: Evaluate the model on the testing set using metrics like accuracy, precision, recall, and F1 score.

In summary, the data analyst's approach to training an Xgboost model with hyperparameter tuning is comprehensive and covers most of the necessary steps. However, there are a few areas where the approach could be refined, like using feature selection techniques, cross-validation techniques, and early stopping techniques. An alternative approach to training an Xgboost model could be using a Neural Network model, which can learn complex patterns in the data.
    </pre>
<h3>Finalized Code</h3>
<pre><code>import pandas as pd
import xgboost as xgb
import optuna
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import concordance_index_censored
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Load the data
data = pd.read_csv("C:\\Users\\HP\\Downloads\\train (1).csv")

# Preprocessing
# Impute missing values for 'Age' and 'Fare', and encode 'Sex', 'Pclass', and 'Embarked'
num_imputer = SimpleImputer(strategy='median')
cat_imputer = SimpleImputer(strategy='most_frequent')
ordinal_encoder = OrdinalEncoder(categories=[['1', '2', '3']])
onehot_encoder = OneHotEncoder()

preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_imputer, ['Age', 'Fare']),
        ('cat', cat_imputer, ['Embarked']),
        ('ordinal', ordinal_encoder, ['Pclass']),
        ('onehot', onehot_encoder, ['Sex'])
    ],
    remainder='drop'
)

data_processed = preprocessor.fit_transform(data)
data_processed = pd.DataFrame(data_processed, columns=['Age', 'Fare', 'Embarked', 'Pclass', 'Sex_male', 'Sex_female'])

# Add Survived back for splitting
data_processed['Survived'] = data['Survived'].values

# Split the data
X = data_processed.drop(columns=['Survived'])
y = data_processed['Survived']

# Define objective for Optuna
def objective(trial):
    param = {
        'objective': 'survival:cox', 
        'eval_metric': 'cox-nloglik',
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),
        'max_depth': trial.suggest_int('max_depth', 3, 9),
        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),
        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),
        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)
    }
    
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    model = xgb.train(param, dtrain, evals=[(dvalid, "validation")], early_stopping_rounds=50, verbose_eval=False)
    preds = model.predict(dvalid)
    ci = concordance_index_censored(y_valid.values.astype(bool), preds)[0]
    return ci

# Run Optuna optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# Train final model
final_model = xgb.XGBRegressor(
    **study.best_params,
    objective='survival:cox'
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
final_model.fit(X_train, y_train)

# Evaluate the model
predictions = final_model.predict(X_test)
ci_score = concordance_index_censored(y_test.astype(bool), predictions)[0]
print(f'Final CI score: {ci_score}')
</code></pre>

<h3>Correlation Matrix</h3>
<img src="correlation_matrix.jpg" alt="Correlation Matrix" width="100%">
</body>
</html>
